{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Knowledge Base Ingestion\n",
        "\n",
        "This notebook ingests research PDFs from the `research_papers/` folder, extracts text, chunks it, creates embeddings, and builds a FAISS index for retrieval.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (run once)\n",
        "!pip install -q pdfplumber sentence-transformers faiss-cpu groq streamlit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import modules\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append(str(Path.cwd()))\n",
        "\n",
        "from src.kb.ingest import ingest_knowledge_base\n",
        "import os\n",
        "\n",
        "# Create necessary directories\n",
        "os.makedirs(\"research_papers\", exist_ok=True)\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "print(\"Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Upload PDFs to research_papers folder\n",
        "\n",
        "**Important:** Upload your research PDFs to the `research_papers/` folder. The ingestion will process all PDF files in this folder.\n",
        "\n",
        "You can upload files using:\n",
        "- Colab: Files → Upload to session storage → Select `research_papers/` folder\n",
        "- Or use the file browser on the left sidebar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List PDFs in research_papers folder\n",
        "from pathlib import Path\n",
        "\n",
        "pdf_folder = Path(\"research_papers\")\n",
        "pdf_files = list(pdf_folder.glob(\"*.pdf\"))\n",
        "\n",
        "print(f\"Found {len(pdf_files)} PDF files:\")\n",
        "for pdf in pdf_files:\n",
        "    print(f\"  - {pdf.name}\")\n",
        "    \n",
        "if len(pdf_files) == 0:\n",
        "    print(\"\\n⚠️  No PDFs found! Please upload PDFs to the research_papers/ folder.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Run Ingestion\n",
        "\n",
        "This will:\n",
        "1. Extract text from all PDFs\n",
        "2. Chunk text into ~400-word chunks with 80-word overlap\n",
        "3. Generate embeddings using sentence-transformers (all-MiniLM-L6-v2)\n",
        "4. Build FAISS index\n",
        "5. Save index and metadata to `data/` folder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run ingestion\n",
        "ingest_knowledge_base(\n",
        "    pdf_folder=\"./research_papers\",\n",
        "    index_path=\"./data/faiss_index.bin\",\n",
        "    metadata_path=\"./data/metadata.jsonl\",\n",
        "    model_name=\"all-MiniLM-L6-v2\",\n",
        "    index_type=\"flat\"  # Use \"hnsw\" for larger datasets (>10k chunks)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Verify Ingestion\n",
        "\n",
        "Check that the index and metadata files were created successfully.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify files exist\n",
        "import os\n",
        "\n",
        "index_path = \"./data/faiss_index.bin\"\n",
        "metadata_path = \"./data/metadata.jsonl\"\n",
        "\n",
        "print(f\"FAISS index exists: {os.path.exists(index_path)}\")\n",
        "print(f\"Metadata file exists: {os.path.exists(metadata_path)}\")\n",
        "\n",
        "if os.path.exists(metadata_path):\n",
        "    # Count chunks\n",
        "    chunk_count = sum(1 for line in open(metadata_path) if line.strip())\n",
        "    print(f\"\\nTotal chunks created: {chunk_count}\")\n",
        "    \n",
        "    # Show sample chunk\n",
        "    import json\n",
        "    with open(metadata_path, 'r') as f:\n",
        "        first_line = f.readline()\n",
        "        if first_line:\n",
        "            sample_chunk = json.loads(first_line)\n",
        "            print(f\"\\nSample chunk:\")\n",
        "            print(f\"  Paper: {sample_chunk.get('paper_title', 'N/A')}\")\n",
        "            print(f\"  Chunk index: {sample_chunk.get('chunk_index', 'N/A')}\")\n",
        "            print(f\"  Text preview: {sample_chunk.get('text', '')[:200]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Test Retrieval\n",
        "\n",
        "Test that retrieval works correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test retrieval\n",
        "from src.kb.retriever import retrieve_chunks\n",
        "\n",
        "query = \"psychometric predictors of loan repayment\"\n",
        "results = retrieve_chunks(query, k=5)\n",
        "\n",
        "print(f\"Retrieved {len(results)} chunks for query: '{query}'\\n\")\n",
        "for i, chunk in enumerate(results, 1):\n",
        "    print(f\"Chunk {i} (score: {chunk['score']:.4f}):\")\n",
        "    print(f\"  Paper: {chunk['paper_title']}\")\n",
        "    print(f\"  Text: {chunk['text'][:200]}...\")\n",
        "    print()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
